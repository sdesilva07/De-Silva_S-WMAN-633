---
title: "Homework 3"
author: "Sindupa De Silva"
date: "February 24, 2021"
output: html_document
---





y: response variable (1 for success, 0 for failure)
x1: continuous predictor variable
x2: categorical predictor variable with 2 levels: "a" and "b"



1. Fit a logistic regression model that assumes the probability of success is an additive function of variables
x1 and x2.

```{r}

#Import the data
Data <- read.csv("Homework 3 Data.csv")
head(Data)

# Fittting a linear model with additive functions
fit <- glm(y ~ x1 + x2, family = binomial, data = Data)
summary(fit)

```




2. Interpret the effect of variable x1 on the log odds of success. Verify your interpretation in R.

```{r}

# Extract model coefficients
betas <- coef(fit)

# Coefficient of variable x1
betas[2]

```

Variable x1 has a negative effect of 0.167976 for 1-unit change on the log odds ratio of success
To verify this further:

```{r}

# multiply the effect of x1 on the log odds of success by an arbitrary value
effect_1 <- betas[1] + betas[2] * 2 

# multiply x1 with a 1-unit difference of the arbitrary value above
effect_2 <- betas[1] + betas[2] * 3 

# substract the latter by the previous to see the difference of 1-unit change on the log odds ratio of success.
effect_2 - effect_1 

```





3. Interpret the effect of variable x2 on the log odds of success. Verify your interpretation in R.

```{r}

# Coefficient of variable x2
betas[3]

```

Variable x2 has a negative effect of 0.9679244 for 1-unit change on the log odds ratio of success.
To verify this further:

```{r}

# multiply the effect of x2 on the log odds of success by an arbitrary value
effect_1 <- betas[1] + betas[3] * 5 

# multiply x2 with a 1-unit difference of the arbitrary value above
effect_2 <- betas[1] + betas[3] * 6 

# substract the latter by the previous to see the difference of 1-unit change on the log odds ratio of sucesss
effect_2 - effect_1 

```






4. Duplicate the Wald Test and p-values for variables x1 and x2 performed by the glm() function. Do
you reject or fail to reject your null hypothesis?

```{r}

# Wald test for test statistic of variable x1
summary(fit)[['coefficients']]['x1', 'z value']
# or
# test statistic for x1
ts_x1 <- betas[2] / summary(fit)[['coefficients']]['x1', 'Std. Error']
ts_x1

# p value for x1
2 * pnorm(-1 * abs(ts_x1), mean = 0, sd = 1)
          
# pvalue for x1 from full model
summary(fit)[['coefficients']]['x1', 'Pr(>|z|)']



# Wald test for test statistic of variable x2
summary(fit)[['coefficients']]['x2b', 'z value'] 
# or
# test statistic for x2
ts_x2 <- betas[3] / summary(fit)[['coefficients']]['x2b', 'Std. Error']
ts_x2

# p value for x2
2 * pnorm(-1 * abs(ts_x2), mean = 0, sd = 1)

# pvalue for x2 from full model
summary(fit)[['coefficients']]['x2b', 'Pr(>|z|)']

```

- For variable x1, We fail to reject the null hypothesis that variable x1 has no effect (HA: B1 = 0), because the pvalue > 0.05 (0.645)

- For variable x2, We reject the null hypothesis that variable x2 has no effect (HA: B1 = 0) because the pvalue < 0.05 (0.019)





5. Predict and plot the mean probability of success over the range of values of x1.

```{r}

# Range of values of varibale x1
x1 <- seq(from = min(Data$x1), to = max(Data$x1),length.out = 100)

# Mean probability of success of variable x1
y <- betas[1] + betas[2] * x1

# Plot
plot(x = x1, y = plogis(y), ylab = 'Probability of success',
    xlab = 'Variable x1', type = 'l')

```



